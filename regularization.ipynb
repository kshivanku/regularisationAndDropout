{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from reg_utils import plot_decision_boundary, load_2D_dataset, predict_dec\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io\n",
    "from testCases import *\n",
    "\n",
    "get_ipython().magic('matplotlib inline')\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(X, Y, nn_structure, num_iterations=3000, learning_rate=0.01, printCost = False, lambd = 0, dropOutProb = 1):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    #initialize the parameters\n",
    "    parameters = initializieParameters(nn_structure)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        #forward propagation\n",
    "        AL, cache = forwardPropagation(X, parameters, dropOutProb)\n",
    "        \n",
    "        #caculate cost\n",
    "        cost = calculateCost(AL, Y, parameters, lambd)\n",
    "        costs.append(cost)\n",
    "        if printCost and i%100 == 0:\n",
    "            print(\"cost after iteration \" + str(i) + \" is \" + str(cost))\n",
    "        \n",
    "        #backpropagation\n",
    "        grads = backpropagation(AL, Y, cache, dropOutProb, lambd)\n",
    "        \n",
    "        #update parameters\n",
    "        parameters  = updateParameters(parameters, grads, learning_rate)\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "def predict(X, Y, parameters):\n",
    "    A, cache = forwardPropagation(X, parameters)\n",
    "    m = Y.shape[1]\n",
    "    p = np.zeros((1, Y.shape[1]), dtype=float)\n",
    "    for i in range(m):\n",
    "        if A[0][i] >= 0.5:\n",
    "            p[0][i] = 1.0\n",
    "        else:\n",
    "            p[0][i] = 0.0\n",
    "    \n",
    "    print(\"Accuracy: \" + str(np.sum((p == Y)/m)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Parameters\n",
    "parameters = {}\n",
    "def initializeParameters(nn_structure):\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(nn_structure[l], nn_structure[l-1]) * np.sqrt(2/nn_structure[l-1])\n",
    "        parameters[\"b\" + str(l)] = np.zeros(nn_structure[l], 1)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward Propagation\n",
    "def forwardPropagation(X, parameters, dropOutProb):\n",
    "    L = len(parameters)//2\n",
    "    A_prev = X\n",
    "    caches = []\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        A, cache = linearActivationForward(A_prev, W, b, \"relu\", dropOutProb)\n",
    "        caches.append(cache)\n",
    "    \n",
    "    #for the last layer\n",
    "    A_prev = A\n",
    "    W = parameters[\"W\" + str(L)]\n",
    "    b = parameters[\"b\" + str(L)]\n",
    "    AL, cache = linearActivationForward(A_prev, W, b, \"sigmoid\", dropOutProb)\n",
    "    caches.append(cache)\n",
    "    return AL, caches    \n",
    "\n",
    "        \n",
    "def linearActivationForward(A_prev, W, b, activationFunction, dropOutProb):\n",
    "    Z, linearCache = linearForward(A_prev, W, b)\n",
    "    if(activationFunction == \"relu\"):\n",
    "        A_preDrop, activationCache = relu(Z)\n",
    "    else:\n",
    "        A_preDrop, activationCache = sigmoid(Z)\n",
    "    A, dropoutCache = dropout(A_preDrop, dropOutProb)\n",
    "    cache = (linearCache, activationCache, dropoutCache)\n",
    "    return A, cache\n",
    "        \n",
    "\n",
    "def linearForward(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev), b\n",
    "    cache = (A_prev, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "        \n",
    "def sigmoid(Z):\n",
    "    A = 1/(1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def dropout(A_preDrop, dropOutProb):\n",
    "    D = np.random.randn(A_preDrop.shape[0], A_preDrop.shape[1])\n",
    "    D = D < dropOutProb\n",
    "    A = np.multiply(A_preDrop, D)\n",
    "    A = A/dropOutProb\n",
    "    cache = D\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Cost\n",
    "def calculateCost(A, Y, parameters, lambd):\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters)//2\n",
    "    fro_norm = 0.0\n",
    "    for l in range(1, L + 1):\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        fro_norm += np.sum(np.square(W))\n",
    "    cost = (1/m) * (np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log(1-A.T))) + (lambd/(2*m)) * fro_norm\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Backpropagation\n",
    "def backpropagation(AL, Y, caches, dropOutProb, lambd):\n",
    "    grads = {}\n",
    "    m = Y.shape[1]\n",
    "    L = len(caches)\n",
    "    currentCache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)] = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    grads[\"dA\" + str(L)] = backwardsDropout(grads[\"dA\" + str(L)], currentCache[2], dropOutProb)\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linearActivationBackwards(grads[\"dA\" + str(L)], currentCache, \"sigmoid\", dropOutProb, lambd)\n",
    "    for l in reversed(range(1, L - 1)):\n",
    "        currentCache = caches[l]\n",
    "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] = linearActivationBackwards(grads[\"dA\" + str(l + 1)], currentCache, \"relu\", dropOutProb, lambd)\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def backwardsDropout(dA, dropOutCache, dropOutProb):\n",
    "    D = dropOutCache\n",
    "    dA = np.multiply(dA, D)\n",
    "    dA = dA/dropOutProb\n",
    "    return dA\n",
    "\n",
    "def linearActivationBackwards(dA, cache, activationFunction, dropOutProb, lambd):\n",
    "    linearCache, activationCache, dropOutCache = cache\n",
    "    if activateFunction == \"sigmoid\":\n",
    "        dZ = sigmoidBackwards(dA, activationCache)\n",
    "    else:\n",
    "        dZ = reluBackwards(dA, activationCache)\n",
    "    dA_prev, dW, db = linearBackwards(dZ, linearCache, lambd)\n",
    "    dA_prev = backwardsDropout(dA_prev, dropOutCache, dropOutProb)\n",
    "    return dA_prev, dW, db\n",
    "        \n",
    "def sigmoidBackwards(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def reluBackwards(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z<=0] = 0\n",
    "    return dZ\n",
    "\n",
    "def linearBackwards(dZ, cache, lambd):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m) * np.dot(dZ, A_prev.T) + (lambd/m) * W\n",
    "    db = (1/m) * np.sum(dZ, axis = 1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, db, dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Update Parameters\n",
    "def updateParameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\"+ str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\"+ str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
